name: CI

on:
  push:
    branches: [main]
  pull_request:
    # Run on all pull requests, regardless of target branch
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.12"
  UV_VERSION: "0.5.12"
  NODE_VERSION: "20"

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write
  models: read  # Enable GitHub Models for AI inference

jobs:
  # Check if Python files were changed to determine if tests should run
  changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      python: ${{ steps.filter.outputs.python }}
      sql: ${{ steps.filter.outputs.sql }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            python:
              - '**/*.py'
              - 'pyproject.toml'
              - 'setup.py'
              - 'setup.cfg'
              - 'requirements*.txt'
            sql:
              - '**/*.sql'
              - '.sqlfluff'

  lint:
    name: Lint & Code Quality
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run linting checks
        run: make lint

  type-check:
    name: Type Checking
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run MyPy
        run: make type-check

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run security checks
        run: make security

      - name: Upload Bandit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: .bandit-report.json

      - name: Upload Safety report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: safety-report
          path: .safety-report.json

  sql-lint:
    name: SQL Linting
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.sql == 'true' || needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run SQLFluff linting
        run: make sql-lint

  # Canary test - fast, minimal test run to catch obvious failures early
  test-canary:
    name: Canary Test (Ubuntu, Python 3.12)
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: make install

      - name: Run canary tests (minimal, fail-fast)
        timeout-minutes: 3
        run: |
          echo "üê§ Running canary tests with minimal output, 10 error limit, and 15s per-test timeout"
          echo "‚è≠Ô∏è  Excluding integration tests (marked with @pytest.mark.integration)"
          SCRIPTRAG_LOG_LEVEL=ERROR uv run pytest -x -q --tb=short --maxfail=10 --disable-warnings --junit-xml=junit.xml -o log_cli=false --capture=fd --timeout=15 --timeout-method=thread -m "not integration"
        env:
          PYTHONPATH: src
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface canary test results
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Canary Test Results (Ubuntu, Python ${{ env.PYTHON_VERSION }})"

      - name: Upload canary test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-canary-ubuntu-py${{ env.PYTHON_VERSION }}
          path: junit.xml

  # Full test matrix - runs only after canary passes
  test-matrix:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [changes, test-canary]
    if: needs.changes.outputs.python == 'true'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.12", "3.13"]
        exclude:
          # Reduce matrix for faster CI
          - os: macos-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ matrix.python-version }}-

      - name: Install dependencies
        run: make install
        shell: bash

      - name: Windows Debug Info
        if: matrix.os == 'windows-latest'
        shell: pwsh
        run: |
          Write-Host "=== Windows Debug Information ==="
          Write-Host "Temp directory: $env:TEMP"
          Write-Host "Number of CPUs: $env:NUMBER_OF_PROCESSORS"
          Write-Host "Available memory:"
          Get-CimInstance Win32_OperatingSystem | Select-Object TotalVisibleMemorySize, FreePhysicalMemory
          Write-Host "Python location:"
          Get-Command python
          Write-Host "UV location:"
          Get-Command uv
          Write-Host "Current directory contents:"
          Get-ChildItem -Force
          Write-Host "================================="

      - name: Run full test suite
        run: |
          echo "üß™ Running full test suite on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
          echo "‚è≠Ô∏è  Excluding integration tests (they run in a separate job)"
          echo "Starting pytest at $(date)"
          make test PYTEST_ARGS="-m 'not integration'"
          echo "Finished pytest at $(date)"
        shell: bash
        env:
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface failing tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Test Results (${{ matrix.os }}, Python ${{ matrix.python-version }})"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: junit.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}


  docs:
    name: Build Documentation
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies and build documentation
        run: |
          make install
          make docs

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: site/

  build:
    name: Build Distribution
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Build distribution
        run: make build

      - name: Check distribution
        run: make check-dist

      - name: Upload distribution
        uses: actions/upload-artifact@v4
        with:
          name: distribution
          path: dist/

  pre-commit:
    name: Pre-commit Hooks
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install markdownlint-cli
        run: npm install -g markdownlint-cli

      - name: Run pre-commit
        uses: pre-commit/action@v3.0.1

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, test-canary]
    if: needs.changes.outputs.python == 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run integration tests
        run: |
          echo "üß™ Running integration tests with available providers"
          echo "Has GITHUB_TOKEN: ${{ github.token != '' }}"
          echo "Has SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT != '' }}"
          make test PYTEST_ARGS="-m integration -v --tb=short"
        env:
          GITHUB_TOKEN: ${{ github.token }}
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface failing integration tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Integration Test Results"

  all-checks:
    name: All Checks Passed
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, security, test-canary, test-matrix, docs, build, pre-commit]
    if: always()
    outputs:
      all_passed: ${{ steps.check.outputs.all_passed }}
    steps:
      - name: Check if all jobs passed
        id: check
        run: |
          # If no Python changes, automatically pass
          if [[ "${{ needs.changes.outputs.python }}" != "true" ]]; then
            echo "No Python files changed - skipping checks"
            echo "all_passed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check for failures only if Python files were changed
          if [[ "${{ contains(needs.*.result, 'failure') }}" == "true" ]]; then
            echo "One or more jobs failed"
            echo "all_passed=false" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "All jobs passed successfully"
            echo "all_passed=true" >> $GITHUB_OUTPUT
          fi

  # Call the reusable Claude Code Review workflow after CI passes
  claude-review-after-ci:
    name: Claude Code Review (Post-CI)
    needs: [all-checks]
    if: |
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'true'
    permissions:
      contents: read
      pull-requests: write
      issues: read
      id-token: write
      models: read  # Enable GitHub Models for AI inference
    uses: ./.github/workflows/claude-code-review.yml
    with:
      pr_number: "${{ github.event.pull_request.number }}"
    secrets:
      GH_TOKEN_FOR_CLAUDE: ${{ secrets.GH_TOKEN_FOR_CLAUDE }}
      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

  # Analyze test failures when CI fails
  test-failure-diagnostics:
    name: Test Failure Diagnostics
    runs-on: ubuntu-latest
    needs: [all-checks]
    if: |
      always() &&
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'false'
    permissions:
      contents: read
      pull-requests: write
      issues: write
      actions: read
      checks: read
      models: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install gh-workflow-peek extension
        run: |
          # Check if gh-workflow-peek is already installed
          if ! gh extension list | grep -q "trieloff/gh-workflow-peek"; then
            echo "Installing gh-workflow-peek extension..."
            gh extension install trieloff/gh-workflow-peek
          else
            echo "gh-workflow-peek extension already installed"
          fi

          # Verify installation
          gh extension list
        env:
          GH_TOKEN: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}

      - name: Analyze Test Failures and Post Diagnostics
        id: analyze-failures
        continue-on-error: true
        run: |
          # Create a Python script to analyze test failures using GitHub Models API
          cat > analyze_failures.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import subprocess
          import sys
          from typing import Dict, List, Any

          def run_command(cmd: List[str]) -> str:
              """Run a command and return its output."""
              try:
                  result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  return result.stdout
              except subprocess.CalledProcessError as e:
                  print(f"Command failed: {' '.join(cmd)}")
                  print(f"Error: {e.stderr}")
                  return ""

          def get_failed_jobs(run_id: str) -> List[Dict[str, Any]]:
              """Get information about failed jobs in the workflow run."""
              cmd = ["gh", "api", f"repos/${{github.repository}}/actions/runs/{run_id}/jobs"]
              output = run_command(cmd)
              if not output:
                  return []

              data = json.loads(output)
              failed_jobs = []

              for job in data.get("jobs", []):
                  if job.get("conclusion") == "failure":
                      failed_jobs.append({
                          "name": job.get("name", "Unknown"),
                          "id": job.get("id"),
                          "url": job.get("html_url", ""),
                          "steps": job.get("steps", [])
                      })

              return failed_jobs

          def extract_error_patterns(job_id: str) -> str:
              """Extract error patterns from job logs."""
              errors = []

              # Try to get job logs using gh workflow-peek if available
              peek_output = run_command(["gh", "workflow-peek", "${{ github.run_id }}", "--job", str(job_id)])
              if peek_output:
                  # Look for common error patterns
                  for line in peek_output.split('\n'):
                      if any(pattern in line.upper() for pattern in ['ERROR', 'FAILED', 'FAILURE', 'ASSERTION']):
                          errors.append(line.strip())
                          if len(errors) >= 5:  # Limit to 5 error lines per job
                              break

              return '\n'.join(errors) if errors else "Unable to extract specific errors"

          def analyze_failure_type(job_name: str, errors: str) -> Dict[str, str]:
              """Analyze the type of failure and suggest fixes."""
              job_lower = job_name.lower()
              errors_lower = errors.lower()

              if "lint" in job_lower or "ruff" in errors_lower:
                  return {
                      "type": "Linting Error",
                      "icon": "üîß",
                      "fix": "Run `make lint` locally to see and fix linting issues",
                      "quick_fix": "Run `make format` to auto-fix formatting issues"
                  }
              elif "type" in job_lower or "mypy" in errors_lower:
                  return {
                      "type": "Type Checking Error",
                      "icon": "üìù",
                      "fix": "Run `make type-check` locally to see type errors",
                      "quick_fix": "Check function signatures and type annotations"
                  }
              elif "security" in job_lower or "bandit" in errors_lower or "safety" in errors_lower:
                  return {
                      "type": "Security Issue",
                      "icon": "üîí",
                      "fix": "Run `make security` locally to see security issues",
                      "quick_fix": "Review flagged security concerns in the code"
                  }
              elif "test" in job_lower or "pytest" in errors_lower:
                  if "timeout" in errors_lower:
                      return {
                          "type": "Test Timeout",
                          "icon": "‚è±Ô∏è",
                          "fix": "Tests are timing out - check for infinite loops or slow operations",
                          "quick_fix": "Run `make test PYTEST_ARGS='-x --tb=short'` to debug"
                      }
                  else:
                      return {
                          "type": "Test Failure",
                          "icon": "‚ùå",
                          "fix": "Run `make test` locally to reproduce failures",
                          "quick_fix": "Check test assertions and expected values"
                      }
              elif "build" in job_lower:
                  return {
                      "type": "Build Error",
                      "icon": "üì¶",
                      "fix": "Run `make build` locally to reproduce the build error",
                      "quick_fix": "Check dependencies and build configuration"
                  }
              else:
                  return {
                      "type": "CI Failure",
                      "icon": "‚ö†Ô∏è",
                      "fix": "Run `make check-fast` for a quick quality check",
                      "quick_fix": "Review the job logs for specific error messages"
                  }

          def main():
              pr_number = "${{ github.event.pull_request.number }}"
              run_id = "${{ github.run_id }}"
              run_url = f"${{ github.server_url }}/${{ github.repository }}/actions/runs/{run_id}"

              print(f"Analyzing failures for PR #{pr_number}")
              print(f"Workflow run: {run_id}")

              # Get failed jobs
              failed_jobs = get_failed_jobs(run_id)

              if not failed_jobs:
                  print("No failed jobs found or unable to fetch job information")
                  return

              # Build the diagnostic comment
              comment_parts = [
                  f"## üîç CI Failure Diagnostics",
                  "",
                  f"**Workflow Run:** [#{run_id}]({run_url})",
                  f"**Failed Jobs:** {len(failed_jobs)}",
                  "",
                  "### üìä Failure Summary",
                  ""
              ]

              # Analyze each failed job
              for job in failed_jobs:
                  job_name = job['name']
                  job_url = job['url']

                  # Extract errors if possible
                  errors = extract_error_patterns(job['id']) if job['id'] else "No error details available"

                  # Analyze failure type
                  analysis = analyze_failure_type(job_name, errors)

                  comment_parts.extend([
                      f"#### {analysis['icon']} {job_name}",
                      f"**Type:** {analysis['type']}",
                      f"**[View Job Logs]({job_url})**",
                      ""
                  ])

                  if errors != "No error details available" and errors != "Unable to extract specific errors":
                      comment_parts.extend([
                          "<details>",
                          "<summary>Error Details</summary>",
                          "",
                          "```",
                          errors[:500],  # Limit error output
                          "```",
                          "",
                          "</details>",
                          ""
                      ])

                  comment_parts.extend([
                      f"**Fix:** {analysis['fix']}",
                      f"**Quick Fix:** {analysis['quick_fix']}",
                      "",
                      "---",
                      ""
                  ])

              # Add general recommendations
              comment_parts.extend([
                  "### üí° Quick Debugging Steps",
                  "",
                  "1. **Run all quality checks locally:**",
                  "   ```bash",
                  "   make check-fast",
                  "   ```",
                  "",
                  "2. **Fix specific issues:**",
                  "   - Linting: `make format` then `make lint`",
                  "   - Type errors: `make type-check`",
                  "   - Test failures: `make test`",
                  "   - Security: `make security`",
                  "",
                  "3. **Common issues to check:**",
                  "   - ANSI codes in test output (use `strip_ansi_codes()`)",
                  "   - Mock file artifacts (use `spec_set` in mocks)",
                  "   - Cross-platform path issues (use `pathlib.Path`)",
                  "   - Type annotations for async functions",
                  "",
                  "---",
                  "",
                  "_This is an automated diagnostic report. For detailed analysis, check the [workflow logs](" + run_url + ")._"
              ])

              comment = '\n'.join(comment_parts)

              # Write comment to file for GitHub command
              with open('comment.md', 'w') as f:
                  f.write(comment)

              # Post the comment
              print("Posting diagnostic comment to PR...")
              result = subprocess.run(
                  ["gh", "pr", "comment", pr_number, "--body-file", "comment.md"],
                  capture_output=True,
                  text=True
              )

              if result.returncode == 0:
                  print("Diagnostic comment posted successfully")
              else:
                  print(f"Failed to post comment: {result.stderr}")
                  sys.exit(1)

          if __name__ == "__main__":
              main()
          EOF

          # Make the script executable and run it
          chmod +x analyze_failures.py
          python3 analyze_failures.py
        env:
          GH_TOKEN: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}

  vibe-badge:
    name: Generate Vibe Badge
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Generate Vibe Badge
        uses: trieloff/vibe-coded-badge-action@main
        continue-on-error: true
