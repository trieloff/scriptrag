name: CI

on:
  push:
    branches: [main]
  pull_request:
    # Run on all pull requests, regardless of target branch
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.12"
  UV_VERSION: "0.5.12"
  NODE_VERSION: "20"

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write
  models: read  # Enable GitHub Models for AI inference

jobs:
  # Check if Python files were changed to determine if tests should run
  changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      python: ${{ steps.filter.outputs.python }}
      sql: ${{ steps.filter.outputs.sql }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            python:
              - '**/*.py'
              - 'pyproject.toml'
              - 'setup.py'
              - 'setup.cfg'
              - 'requirements*.txt'
            sql:
              - '**/*.sql'
              - '.sqlfluff'

  lint:
    name: Lint & Code Quality
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run linting checks
        run: make lint

  type-check:
    name: Type Checking
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run MyPy
        run: make type-check

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run security checks
        run: make security

      - name: Upload Bandit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: .bandit-report.json

      - name: Upload Safety report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: safety-report
          path: .safety-report.json

  sql-lint:
    name: SQL Linting
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.sql == 'true' || needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run SQLFluff linting
        run: make sql-lint

  # Canary test - fast, minimal test run to catch obvious failures early
  test-canary:
    name: Canary Test (Ubuntu, Python 3.12)
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: make install

      - name: Run canary tests (minimal, fail-fast)
        timeout-minutes: 5
        run: |
          echo " Running canary tests with minimal output, 10 error limit, and 15s per-test timeout"
          echo "  Excluding integration tests (marked with @pytest.mark.integration)"
          SCRIPTRAG_LOG_LEVEL=ERROR uv run pytest -x -q --tb=short --maxfail=10 --disable-warnings --junit-xml=junit.xml -o log_cli=false --capture=fd --timeout=15 --timeout-method=thread -m "not integration"
        env:
          PYTHONPATH: src
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing
          SCRIPTRAG_TEST_LLMS: ""  # Disable LLM tests in CI to prevent rate limit failures

      - name: Check for mock file contamination (canary)
        if: always()
        run: |
          echo " Checking for mock file contamination after canary tests..."
          if python tests/detect_mock_files.py --quiet; then
            echo " No mock files detected in canary tests"
          else
            echo "WARNING: Mock files detected in canary tests - early warning of test issues"
            python tests/detect_mock_files.py
          fi
        continue-on-error: true

      - name: Surface canary test results
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Canary Test Results (Ubuntu, Python ${{ env.PYTHON_VERSION }})"

      - name: Upload canary test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-canary-ubuntu-py${{ env.PYTHON_VERSION }}
          path: junit.xml

  # Full test matrix - runs only after canary passes
  test-matrix:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [changes, test-canary]
    if: needs.changes.outputs.python == 'true'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.12", "3.13"]
        exclude:
          # Reduce matrix for faster CI
          - os: macos-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ matrix.python-version }}-

      - name: Install dependencies
        run: make install
        shell: bash

      - name: Windows Debug Info
        if: matrix.os == 'windows-latest'
        shell: pwsh
        run: |
          Write-Host "=== Windows Debug Information ==="
          Write-Host "Temp directory: $env:TEMP"
          Write-Host "Number of CPUs: $env:NUMBER_OF_PROCESSORS"
          Write-Host "Available memory:"
          Get-CimInstance Win32_OperatingSystem | Select-Object TotalVisibleMemorySize, FreePhysicalMemory
          Write-Host "Python location:"
          Get-Command python
          Write-Host "UV location:"
          Get-Command uv
          Write-Host "Current directory contents:"
          Get-ChildItem -Force
          Write-Host "================================="

      - name: Check for mock file contamination (pre-test)
        run: |
          echo " Checking for pre-existing mock file contamination..."
          python tests/detect_mock_files.py --quiet || true
        shell: bash
        continue-on-error: true

      - name: Run full test suite
        id: run-tests
        run: |
          echo " Running full test suite on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
          echo "  Excluding integration tests (they run in a separate job)"
          echo "Starting pytest at $(date)"
          make test PYTEST_ARGS="-m 'not integration'"
          TEST_EXIT_CODE=$?
          echo "Finished pytest at $(date)"
          echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
          exit $TEST_EXIT_CODE
        shell: bash
        env:
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing
          SCRIPTRAG_TEST_LLMS: ""  # Disable LLM tests in CI to prevent rate limit failures
        continue-on-error: true

      - name: Check for mock file contamination (post-test)
        id: mock-check
        if: always()
        run: |
          echo " Checking for mock file contamination after tests..."
          if python tests/detect_mock_files.py; then
            echo " No mock files detected"
            echo "mock_contamination=false" >> $GITHUB_OUTPUT
          else
            echo "ERROR: Tests created mock file artifacts!"
            echo "mock_contamination=true" >> $GITHUB_OUTPUT

            # Show the mock files found
            python tests/detect_mock_files.py

            echo ""
            echo "These files indicate improper mock configuration in tests."
            echo "Mock objects should return proper string values, not Mock object representations."
            echo ""
            echo "To fix: Ensure all mocked methods that return paths return actual string values."
            echo "Example: mock_settings.database_path = '/test/db.sqlite'"
            echo ""
            echo "Run 'make test-trace-mocks' locally to identify problematic tests"

            # If tests failed AND mock files exist, it's likely the cause
            if [ "${{ steps.run-tests.outputs.test_exit_code }}" != "0" ]; then
              echo ""
              echo "WARNING: Tests failed AND mock files were detected - this is likely the cause of test failures!"
              exit 1
            fi
          fi
        shell: bash

      - name: Run mock contamination audit if mock files detected
        if: |
          always() &&
          steps.mock-check.outputs.mock_contamination == 'true'
        run: |
          echo " Running detailed mock contamination audit..."
          echo "Mock files were detected - investigating which tests are responsible"
          echo ""

          # Clean up any existing mock files first
          python tests/detect_mock_files.py --clean || true

          # First, narrow down to directory
          echo "Step 1: Identifying which test directory creates mock files..."
          PROBLEMATIC_DIR=""
          for test_dir in tests/unit tests/integration tests/cli; do
            if [ -d "$test_dir" ]; then
              echo "  Testing $test_dir..."
              python tests/detect_mock_files.py --clean 2>/dev/null || true

              if uv run pytest "$test_dir" -q --tb=no -x 2>/dev/null; then
                if ! python tests/detect_mock_files.py --quiet 2>/dev/null; then
                  echo "    WARNING: Mock files detected in $test_dir"
                  PROBLEMATIC_DIR="$test_dir"
                  python tests/detect_mock_files.py --clean 2>/dev/null || true
                  break
                fi
              else
                echo "    ERROR: Tests failed in $test_dir"
              fi

              python tests/detect_mock_files.py --clean 2>/dev/null || true
            fi
          done

          # If we found a problematic directory, narrow down to specific file
          if [ -n "$PROBLEMATIC_DIR" ]; then
            echo ""
            echo "Step 2: Identifying specific test file in $PROBLEMATIC_DIR..."

            # Clean any existing mock files from Step 1 before testing individual files
            python tests/detect_mock_files.py --clean 2>/dev/null || true

            # Test each file individually
            for test_file in $(find "$PROBLEMATIC_DIR" -name "test_*.py" -type f | sort); do
              echo "  Testing $test_file..."
              python tests/detect_mock_files.py --clean 2>/dev/null || true

              # Run just this one test file
              if uv run pytest "$test_file" -q --tb=no 2>/dev/null; then
                if ! python tests/detect_mock_files.py --quiet 2>/dev/null; then
                  echo "    🎯 FOUND: $test_file creates mock file artifacts!"
                  python tests/detect_mock_files.py 2>/dev/null || true

                  # Try to identify specific test function
                  echo ""
                  echo "Step 3: Attempting to identify specific test function..."
                  python tests/detect_mock_files.py --clean 2>/dev/null || true

                  # Get list of test functions
                  TEST_FUNCTIONS=$(uv run pytest "$test_file" --collect-only -q 2>/dev/null | grep "<Function" | sed 's/.*<Function \(.*\)>/\1/' || true)

                  if [ -n "$TEST_FUNCTIONS" ]; then
                    for test_func in $TEST_FUNCTIONS; do
                      echo "    Testing $test_file::$test_func"
                      python tests/detect_mock_files.py --clean 2>/dev/null || true

                      if uv run pytest "$test_file::$test_func" -q --tb=no 2>/dev/null; then
                        if ! python tests/detect_mock_files.py --quiet 2>/dev/null; then
                          echo "      ERROR: $test_func creates mock files!"
                          python tests/detect_mock_files.py --clean 2>/dev/null || true
                        fi
                      fi
                    done
                  fi

                  break
                fi
              else
                echo "    Test failed (may be unrelated to mock files)"
              fi

              python tests/detect_mock_files.py --clean 2>/dev/null || true
            done
          fi

          echo ""
          echo "📝 Summary:"
          echo "Mock file contamination detected. This typically happens when:"
          echo "1. MagicMock() is created without spec parameter"
          echo "2. Mock's database_path attribute is not set to a proper string/Path"
          echo "3. Mock objects are used directly as file paths"
          echo ""
          echo "To fix: Add spec to MagicMock() and ensure database_path returns a proper string"
          echo "Example: mock_settings.database_path = Path('/test/db.sqlite')"
        shell: bash
        continue-on-error: true

      - name: Fail build if tests failed
        if: steps.run-tests.outputs.test_exit_code != '0'
        run: |
          echo "ERROR: Tests failed with exit code ${{ steps.run-tests.outputs.test_exit_code }}"
          if [ "${{ steps.mock-check.outputs.mock_contamination }}" == "true" ]; then
            echo "Mock file contamination was detected - this is likely the cause"
          fi
          exit ${{ steps.run-tests.outputs.test_exit_code }}
        shell: bash

      - name: Surface failing tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Test Results (${{ matrix.os }}, Python ${{ matrix.python-version }})"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: junit.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}


  docs:
    name: Build Documentation
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies and build documentation
        run: |
          make install
          make docs

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: site/

  build:
    name: Build Distribution
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Build distribution
        run: make build

      - name: Check distribution
        run: make check-dist

      - name: Upload distribution
        uses: actions/upload-artifact@v4
        with:
          name: distribution
          path: dist/

  pre-commit:
    name: Pre-commit Hooks
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install markdownlint-cli
        run: npm install -g markdownlint-cli

      - name: Run pre-commit
        uses: pre-commit/action@v3.0.1

  mock-detection:
    name: Mock File Detection Check
    runs-on: ubuntu-latest
    needs: [test-matrix]
    if: always() && needs.test-matrix.result != 'skipped'
    continue-on-error: true  # Don't fail the build yet, this is informational

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run comprehensive mock detection
        id: mock-check
        run: |
          echo " Running comprehensive mock file detection..."
          echo "This helps identify tests that create filesystem artifacts"
          echo ""

          # Run the CI detection script with GitHub annotations
          if python tests/ci_mock_detection.py \
            --github-annotations \
            --junit mock-detection-report.xml \
            --test-args "-m 'not integration' -k 'not test_llm' --tb=no -q"; then
            echo " No mock files detected during test run"
            echo "mock_files_found=false" >> $GITHUB_OUTPUT
          else
            EXIT_CODE=$?
            if [ $EXIT_CODE -eq 2 ]; then
              echo "WARNING: Mock files detected - see report above"
              echo "mock_files_found=true" >> $GITHUB_OUTPUT
            else
              echo "ERROR: Tests failed during mock detection run"
              echo "mock_files_found=unknown" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Upload mock detection report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mock-detection-report
          path: mock-detection-report.xml

      - name: Post PR comment if mock files found
        if: github.event_name == 'pull_request' && steps.mock-check.outputs.mock_files_found == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const comment = `## WARNING: Mock File Detection Warning

            Tests in this PR are creating MagicMock file artifacts in the filesystem.

            ### How to investigate locally:

            1. Run \`make test-trace-mocks\` to identify which specific tests create mock files
            2. Fix the tests using the patterns in \`docs/MOCK_FILE_DETECTION.md\`

            ### Common fixes:

            - Always use \`spec_set=True\` when creating mocks
            - Never use mock objects directly as file paths
            - Use \`tmp_path\` fixture for file operations in tests

            This check is informational and won't fail the build (yet).`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, test-canary]
    if: needs.changes.outputs.python == 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run integration tests
        run: |
          echo " Running integration tests with available providers"
          echo "Has GITHUB_TOKEN: ${{ github.token != '' }}"
          echo "Has SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT != '' }}"
          make test PYTEST_ARGS="-m integration -v --tb=short"
        env:
          GITHUB_TOKEN: ${{ github.token }}
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing
          SCRIPTRAG_TEST_LLMS: ""  # Explicitly disable LLM tests to prevent rate limit failures

      - name: Surface failing integration tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Integration Test Results"

  all-checks:
    name: All Checks Passed
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, security, test-canary, test-matrix, docs, build, pre-commit]
    if: always()
    outputs:
      all_passed: ${{ steps.check.outputs.all_passed }}
    steps:
      - name: Check if all jobs passed
        id: check
        run: |
          # If no Python changes, automatically pass
          if [[ "${{ needs.changes.outputs.python }}" != "true" ]]; then
            echo "No Python files changed - skipping checks"
            echo "all_passed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check for failures only if Python files were changed
          if [[ "${{ contains(needs.*.result, 'failure') }}" == "true" ]]; then
            echo "One or more jobs failed"
            echo "all_passed=false" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "All jobs passed successfully"
            echo "all_passed=true" >> $GITHUB_OUTPUT
          fi

  # Call the reusable Claude Code Review workflow after CI passes
  claude-review-after-ci:
    name: Claude Code Review (Post-CI)
    needs: [all-checks]
    if: |
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'true'
    permissions:
      contents: read
      pull-requests: write
      issues: read
      id-token: write
      models: read  # Enable GitHub Models for AI inference
    uses: ./.github/workflows/claude-code-review.yml
    with:
      pr_number: "${{ github.event.pull_request.number }}"
    secrets:
      GH_TOKEN_FOR_CLAUDE: ${{ secrets.GH_TOKEN_FOR_CLAUDE }}
      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

  # Analyze test failures when CI fails
  test-failure-diagnostics:
    name: Test Failure Diagnostics
    runs-on: ubuntu-latest
    needs: [all-checks]
    if: |
      always() &&
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'false'
    permissions:
      contents: read
      pull-requests: write
      issues: write
      actions: read
      checks: read
      models: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install gh-workflow-peek extension
        run: |
          # Check if gh-workflow-peek is already installed
          if ! gh extension list | grep -q "trieloff/gh-workflow-peek"; then
            echo "Installing gh-workflow-peek extension..."
            gh extension install trieloff/gh-workflow-peek
          else
            echo "gh-workflow-peek extension already installed"
          fi

          # Verify installation
          gh extension list
        env:
          GH_TOKEN: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}

      - name: Run Claude Code for Diagnostic Analysis
        id: claude-diagnostics
        continue-on-error: true
        uses: anthropics/claude-code-action@beta
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          github_token: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}
          mode: agent  # Use agent mode for automated diagnostics

          # Pass context via environment variables
          claude_env: |
            PR_NUMBER: ${{ github.event.pull_request.number }}
            PR_BRANCH: ${{ github.head_ref }}
            WORKFLOW_RUN_ID: ${{ github.run_id }}
            WORKFLOW_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          # Grant Claude access to CI logs and tools
          additional_permissions: |
            actions: read
            checks: read

          # Allow Claude to use workflow analysis tools
          allowed_tools: |
            Bash(gh workflow-peek)
            Bash(gh api)
            Bash(gh run view)
            Bash(gh run download)

          # Custom diagnostic prompt
          direct_prompt: |
            ## Test Failure Diagnostic Analysis

            The CI workflow has failed for PR #${{ github.event.pull_request.number }}.
            Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

            Your task is to analyze the test failures and provide a detailed diagnostic report.

            ### Instructions:

            1. **Retrieve Failure Information**:
               - Use the GitHub API or gh CLI to access the current workflow run ID: ${{ github.run_id }}
               - Use `gh workflow-peek` extension to get detailed logs
               - Focus on jobs that failed: lint, type-check, security, test-canary, test-matrix, etc.

            2. **Analyze the Failures**:
               - Identify which specific tests or checks failed
               - Extract relevant error messages and stack traces
               - Look for patterns in the failures (e.g., all failures in one module, timeout issues, dependency problems)
               - Check if failures are related to:
                 * Code syntax errors (linting)
                 * Type checking issues (mypy)
                 * Security violations (bandit/safety)
                 * Test assertion failures
                 * Platform-specific issues (Windows/macOS/Linux differences)

            3. **Root Cause Analysis**:
               - Determine the most likely root cause(s) of the failures
               - Check if the failures are related to recent code changes in the PR
               - Consider common issues from CLAUDE.md:
                 * ANSI escape sequences in test output
                 * Mock file artifacts (check mock-detection job results)
                 * LLM rate limits
                 * Cross-platform compatibility

            4. **Provide Actionable Recommendations**:
               - Suggest specific fixes for each identified issue
               - Include the exact Make commands to run locally:
                 * `make check-fast` for quick quality checks
                 * `make lint` for linting issues
                 * `make type-check` for type errors
                 * `make test` for test failures
               - Reference relevant documentation (TESTING.md, CLAUDE.md)

            5. **Format Your Response**:
               Post a clear, structured comment on the PR with:
               - 📊 **Summary**: Brief overview of what failed
               -  **Detailed Analysis**: Specific failures and their causes
               - 💡 **Recommendations**: Step-by-step fixes with commands
               - 📝 **Additional Notes**: Platform-specific issues or known gotchas

            ### Important:
            After analyzing the failures, post your diagnostic report as a comment on PR #${{ github.event.pull_request.number }} using:
            ```bash
            gh pr comment ${{ github.event.pull_request.number }} --body "Your formatted diagnostic report here"
            ```

            Focus on being helpful and actionable. Remember common issues from the project:
            - Strip ANSI codes in CLI tests using `strip_ansi_codes()`
            - Use `spec_set` in mocks to avoid file artifacts (check mock-detection job)
            - Run `make test-trace-mocks` locally to identify tests creating mock files
            - Handle LLM rate limits with exponential backoff
            - Use `pathlib.Path` for cross-platform paths

          # Custom instructions for diagnostic behavior
          custom_instructions: |
            You are a CI/CD diagnostic specialist for the ScriptRAG project. Your role is to:
            - Quickly identify and analyze test failures specific to this codebase
            - Reference the project's CLAUDE.md and TESTING.md documentation
            - Provide clear, actionable diagnostic information with exact commands
            - Be aware of common issues: ANSI codes, mock artifacts, type annotations, cross-platform issues
            - Suggest using appropriate sub-agents when needed (ruff-house, type-veronica, test-holmes)
            - Be encouraging and constructive in your feedback

            Remember to check for the specific patterns mentioned in CLAUDE.md's "Common Iteration Patterns" section.

  vibe-badge:
    name: Generate Vibe Badge
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Generate Vibe Badge
        uses: trieloff/vibe-coded-badge-action@main
        continue-on-error: true
