name: CI

on:
  push:
    branches: [main]
  pull_request:
    # Run on all pull requests, regardless of target branch
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.12"
  UV_VERSION: "0.5.12"
  NODE_VERSION: "20"

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write
  models: read  # Enable GitHub Models for AI inference

jobs:
  # Check if Python files were changed to determine if tests should run
  changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      python: ${{ steps.filter.outputs.python }}
      sql: ${{ steps.filter.outputs.sql }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            python:
              - '**/*.py'
              - 'pyproject.toml'
              - 'setup.py'
              - 'setup.cfg'
              - 'requirements*.txt'
            sql:
              - '**/*.sql'
              - '.sqlfluff'

  lint:
    name: Lint & Code Quality
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run linting checks
        run: make lint

  type-check:
    name: Type Checking
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run MyPy
        run: make type-check

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run security checks
        run: make security

      - name: Upload Bandit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: .bandit-report.json

      - name: Upload Safety report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: safety-report
          path: .safety-report.json

  sql-lint:
    name: SQL Linting
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.sql == 'true' || needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run SQLFluff linting
        run: make sql-lint

  # Canary test - fast, minimal test run to catch obvious failures early
  test-canary:
    name: Canary Test (Ubuntu, Python 3.12)
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: make install

      - name: Run canary tests (minimal, fail-fast)
        timeout-minutes: 3
        run: |
          echo "üê§ Running canary tests with minimal output, 10 error limit, and 15s per-test timeout"
          echo "‚è≠Ô∏è  Excluding integration tests (marked with @pytest.mark.integration)"
          SCRIPTRAG_LOG_LEVEL=ERROR uv run pytest -x -q --tb=short --maxfail=10 --disable-warnings --junit-xml=junit.xml -o log_cli=false --capture=fd --timeout=15 --timeout-method=thread -m "not integration"
        env:
          PYTHONPATH: src
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface canary test results
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Canary Test Results (Ubuntu, Python ${{ env.PYTHON_VERSION }})"

      - name: Upload canary test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-canary-ubuntu-py${{ env.PYTHON_VERSION }}
          path: junit.xml

  # Full test matrix - runs only after canary passes
  test-matrix:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [changes, test-canary]
    if: needs.changes.outputs.python == 'true'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.12", "3.13"]
        exclude:
          # Reduce matrix for faster CI
          - os: macos-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: uv-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-py${{ matrix.python-version }}-

      - name: Install dependencies
        run: make install
        shell: bash

      - name: Windows Debug Info
        if: matrix.os == 'windows-latest'
        shell: pwsh
        run: |
          Write-Host "=== Windows Debug Information ==="
          Write-Host "Temp directory: $env:TEMP"
          Write-Host "Number of CPUs: $env:NUMBER_OF_PROCESSORS"
          Write-Host "Available memory:"
          Get-CimInstance Win32_OperatingSystem | Select-Object TotalVisibleMemorySize, FreePhysicalMemory
          Write-Host "Python location:"
          Get-Command python
          Write-Host "UV location:"
          Get-Command uv
          Write-Host "Current directory contents:"
          Get-ChildItem -Force
          Write-Host "================================="

      - name: Run full test suite
        run: |
          echo "üß™ Running full test suite on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
          echo "‚è≠Ô∏è  Excluding integration tests (they run in a separate job)"
          echo "Starting pytest at $(date)"
          make test PYTEST_ARGS="-m 'not integration'"
          echo "Finished pytest at $(date)"
        shell: bash
        env:
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface failing tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Test Results (${{ matrix.os }}, Python ${{ matrix.python-version }})"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: junit.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}


  docs:
    name: Build Documentation
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies and build documentation
        run: |
          make install
          make docs

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: site/

  build:
    name: Build Distribution
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.python == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Build distribution
        run: make build

      - name: Check distribution
        run: make check-dist

      - name: Upload distribution
        uses: actions/upload-artifact@v4
        with:
          name: distribution
          path: dist/

  pre-commit:
    name: Pre-commit Hooks
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install markdownlint-cli
        run: npm install -g markdownlint-cli

      - name: Run pre-commit
        uses: pre-commit/action@v3.0.1

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, test-canary]
    if: needs.changes.outputs.python == 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Install dependencies
        run: make install

      - name: Run integration tests
        run: |
          echo "üß™ Running integration tests with available providers"
          echo "Has GITHUB_TOKEN: ${{ github.token != '' }}"
          echo "Has SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT != '' }}"
          make test PYTEST_ARGS="-m integration -v --tb=short"
        env:
          GITHUB_TOKEN: ${{ github.token }}
          SCRIPTRAG_LLM_ENDPOINT: ${{ secrets.SCRIPTRAG_LLM_ENDPOINT }}
          SCRIPTRAG_LLM_API_KEY: ${{ secrets.SCRIPTRAG_LLM_API_KEY }}
          SCRIPTRAG_LLM_DEFAULT_MODEL: ${{ secrets.SCRIPTRAG_LLM_DEFAULT_MODEL || 'default' }}
          SCRIPTRAG_ENVIRONMENT: testing

      - name: Surface failing integration tests
        uses: pmeier/pytest-results-action@main
        if: always()
        with:
          path: junit.xml
          summary: true
          display-options: fEsX
          fail-on-empty: true
          title: "Integration Test Results"

  all-checks:
    name: All Checks Passed
    runs-on: ubuntu-latest
    needs: [changes, lint, type-check, security, test-canary, test-matrix, docs, build, pre-commit]
    if: always()
    outputs:
      all_passed: ${{ steps.check.outputs.all_passed }}
    steps:
      - name: Check if all jobs passed
        id: check
        run: |
          # If no Python changes, automatically pass
          if [[ "${{ needs.changes.outputs.python }}" != "true" ]]; then
            echo "No Python files changed - skipping checks"
            echo "all_passed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check for failures only if Python files were changed
          if [[ "${{ contains(needs.*.result, 'failure') }}" == "true" ]]; then
            echo "One or more jobs failed"
            echo "all_passed=false" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "All jobs passed successfully"
            echo "all_passed=true" >> $GITHUB_OUTPUT
          fi

  # Call the reusable Claude Code Review workflow after CI passes
  claude-review-after-ci:
    name: Claude Code Review (Post-CI)
    needs: [all-checks]
    if: |
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'true'
    permissions:
      contents: read
      pull-requests: write
      issues: read
      id-token: write
      models: read  # Enable GitHub Models for AI inference
    uses: ./.github/workflows/claude-code-review.yml
    with:
      pr_number: "${{ github.event.pull_request.number }}"
    secrets:
      GH_TOKEN_FOR_CLAUDE: ${{ secrets.GH_TOKEN_FOR_CLAUDE }}
      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

  # Analyze test failures when CI fails
  test-failure-diagnostics:
    name: Test Failure Diagnostics
    runs-on: ubuntu-latest
    needs: [all-checks]
    if: |
      always() &&
      github.event_name == 'pull_request' &&
      needs.all-checks.outputs.all_passed == 'false'
    permissions:
      contents: read
      pull-requests: write
      issues: write
      actions: read
      checks: read
      models: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install gh-workflow-peek extension
        run: |
          # Check if gh-workflow-peek is already installed
          if ! gh extension list | grep -q "trieloff/gh-workflow-peek"; then
            echo "Installing gh-workflow-peek extension..."
            gh extension install trieloff/gh-workflow-peek
          else
            echo "gh-workflow-peek extension already installed"
          fi

          # Verify installation
          gh extension list
        env:
          GH_TOKEN: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}

      - name: Run Claude Code for Diagnostic Analysis
        id: claude-diagnostics
        continue-on-error: true
        uses: anthropics/claude-code-action@beta
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          github_token: ${{ secrets.GH_TOKEN_FOR_CLAUDE || github.token }}
          mode: agent  # Use agent mode for automated diagnostics

          # Pass context via environment variables
          claude_env: |
            PR_NUMBER: ${{ github.event.pull_request.number }}
            PR_BRANCH: ${{ github.head_ref }}
            WORKFLOW_RUN_ID: ${{ github.run_id }}
            WORKFLOW_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          # Grant Claude access to CI logs and tools
          additional_permissions: |
            actions: read
            checks: read

          # Allow Claude to use workflow analysis tools
          allowed_tools: |
            Bash(gh workflow-peek)
            Bash(gh api)
            Bash(gh run view)
            Bash(gh run download)

          # Custom diagnostic prompt
          direct_prompt: |
            ## Test Failure Diagnostic Analysis

            The CI workflow has failed for PR #${{ github.event.pull_request.number }}.
            Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

            Your task is to analyze the test failures and provide a detailed diagnostic report.

            ### Instructions:

            1. **Retrieve Failure Information**:
               - Use the GitHub API or gh CLI to access the current workflow run ID: ${{ github.run_id }}
               - Use `gh workflow-peek` extension to get detailed logs
               - Focus on jobs that failed: lint, type-check, security, test-canary, test-matrix, etc.

            2. **Analyze the Failures**:
               - Identify which specific tests or checks failed
               - Extract relevant error messages and stack traces
               - Look for patterns in the failures (e.g., all failures in one module, timeout issues, dependency problems)
               - Check if failures are related to:
                 * Code syntax errors (linting)
                 * Type checking issues (mypy)
                 * Security violations (bandit/safety)
                 * Test assertion failures
                 * Platform-specific issues (Windows/macOS/Linux differences)

            3. **Root Cause Analysis**:
               - Determine the most likely root cause(s) of the failures
               - Check if the failures are related to recent code changes in the PR
               - Consider common issues from CLAUDE.md:
                 * ANSI escape sequences in test output
                 * Mock file artifacts
                 * LLM rate limits
                 * Cross-platform compatibility

            4. **Provide Actionable Recommendations**:
               - Suggest specific fixes for each identified issue
               - Include the exact Make commands to run locally:
                 * `make check-fast` for quick quality checks
                 * `make lint` for linting issues
                 * `make type-check` for type errors
                 * `make test` for test failures
               - Reference relevant documentation (TESTING.md, CLAUDE.md)

            5. **Format Your Response**:
               Post a clear, structured comment on the PR with:
               - üìä **Summary**: Brief overview of what failed
               - üîç **Detailed Analysis**: Specific failures and their causes
               - üí° **Recommendations**: Step-by-step fixes with commands
               - üìù **Additional Notes**: Platform-specific issues or known gotchas

            ### Important:
            After analyzing the failures, post your diagnostic report as a comment on PR #${{ github.event.pull_request.number }} using:
            ```bash
            gh pr comment ${{ github.event.pull_request.number }} --body "Your formatted diagnostic report here"
            ```

            Focus on being helpful and actionable. Remember common issues from the project:
            - Strip ANSI codes in CLI tests using `strip_ansi_codes()`
            - Use `spec_set` in mocks to avoid file artifacts
            - Handle LLM rate limits with exponential backoff
            - Use `pathlib.Path` for cross-platform paths

          # Custom instructions for diagnostic behavior
          custom_instructions: |
            You are a CI/CD diagnostic specialist for the ScriptRAG project. Your role is to:
            - Quickly identify and analyze test failures specific to this codebase
            - Reference the project's CLAUDE.md and TESTING.md documentation
            - Provide clear, actionable diagnostic information with exact commands
            - Be aware of common issues: ANSI codes, mock artifacts, type annotations, cross-platform issues
            - Suggest using appropriate sub-agents when needed (ruff-house, type-veronica, test-holmes)
            - Be encouraging and constructive in your feedback

            Remember to check for the specific patterns mentioned in CLAUDE.md's "Common Iteration Patterns" section.

  vibe-badge:
    name: Generate Vibe Badge
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Generate Vibe Badge
        uses: trieloff/vibe-coded-badge-action@main
        continue-on-error: true
